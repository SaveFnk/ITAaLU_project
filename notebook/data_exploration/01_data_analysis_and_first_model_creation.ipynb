{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd47a9f-d01b-4896-898f-e97934d4f342",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c99b42f-2ca0-48ec-8733-1c7d2b849310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zan\\Desktop\\NTNU\\TDT4310_Intelligent_text_analysis_language_understanding\\ITAaLU_project\\notebook\\data_exploration\n",
      "C:\\Users\\Zan\\Desktop\\NTNU\\TDT4310_Intelligent_text_analysis_language_understanding\\ITAaLU_project\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import plotly \n",
    "import os\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "try:\n",
    "    print(run_only_once)\n",
    "except Exception as e:\n",
    "    print(os.getcwd())\n",
    "    os.chdir(\"./../..\")\n",
    "    print(os.getcwd())\n",
    "    run_only_once = \"Dir has already been changed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ce5f4a-31b2-4eab-a35a-2d2c391e2abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hatespeech_v2_dataset(save=True):\n",
    "    hate_df = pd.read_csv(\"data/toraman22_hate_speech_v2/Toraman22_hate_speech_v2.tsv\", sep=\"\\t\")\n",
    "    dataset_v2 = pd.read_csv(\"data\\hatespeech_v2\\hate_speech_dataset_v2.csv\")\n",
    "    \n",
    "    print(\"Starting size of full dataset\", hate_df.shape)\n",
    "    hate_df = hate_df.drop_duplicates()\n",
    "    print(\"Removed duplicates size of dataset\", hate_df.shape)\n",
    "    \n",
    "    indexes_to_drop = []\n",
    "    for index, row in hate_df.iterrows():\n",
    "        try:\n",
    "            favourite_int = int(row[\"tweet_id\"])\n",
    "        except:\n",
    "            indexes_to_drop.append(index)\n",
    "    \n",
    "    print(f\"Found {len(indexes_to_drop)} rows to drop\")\n",
    "    hate_df = hate_df.drop(indexes_to_drop)\n",
    "    hate_df[\"tweet_id\"] = hate_df['tweet_id'].astype(str)\n",
    "    new_size = hate_df.shape\n",
    "    print(f\"New size is {new_size}\")\n",
    "\n",
    "    # verify that the IDs are present in the dataset on Github\n",
    "    dataset_v2[\"TweetID\"] = dataset_v2['TweetID'].astype(str)\n",
    "    out_df = hate_df[hate_df[\"tweet_id\"].isin(dataset_v2[\"TweetID\"])]\n",
    "    assert new_size == out_df.shape, \"Problems occured while filtering! Sizes should match!\"\n",
    "    assert 128907 == out_df.shape[0], f\"Size doesn't match with the required one! It should be 128907, but is {out_df.shape[0]}!\"\n",
    "    \n",
    "    # filter by english\n",
    "    out_df = out_df[out_df[\"language\"] == 1].reset_index(drop=True)\n",
    "    assert out_df.shape[0] == 68597, \"Size of the final processed dataset isn't correct, errors occured!\"\n",
    "    print(f\"Final dataset size is {out_df.shape}\")\n",
    "\n",
    "    # remove large spaces\n",
    "    out_df[\"text\"] = out_df[\"text\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    if save:\n",
    "        save_file = \"data/hatespeech_v2/prepared_hatespeech_v2.csv\"\n",
    "        print(\"Saving data\")\n",
    "        out_df.to_csv(save_file, index=False)\n",
    "        print(f\"Saved preprocessed data to: {save_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09419632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zan\\AppData\\Local\\Temp\\ipykernel_7812\\449982524.py:2: DtypeWarning: Columns (0,4,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  hate_df = pd.read_csv(\"data/toraman22_hate_speech_v2/Toraman22_hate_speech_v2.tsv\", sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting size of full dataset (162386, 20)\n",
      "Removed duplicates size of dataset (128917, 20)\n",
      "Found 10 rows to drop\n",
      "New size is (128907, 20)\n",
      "Final dataset size is (68597, 20)\n",
      "Saving data\n",
      "Saved preprocessed data to: data/hatespeech_v2/prepared_hatespeech_v2.csv\n"
     ]
    }
   ],
   "source": [
    "prepare_hatespeech_v2_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac3023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba53050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4e558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d00b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e95a3a-3dfa-4ed7-b916-5221ed4839c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bca7e-bb15-4dd2-bdd0-05ceb120f379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9a909-2758-4bb7-b081-cbce7b2d38a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3392a99-e174-4f67-a6b5-9e1e2f60b81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f8d26-a511-44d1-a684-6bcf4e84f9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
