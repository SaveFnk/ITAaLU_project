{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd47a9f-d01b-4896-898f-e97934d4f342",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c99b42f-2ca0-48ec-8733-1c7d2b849310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T16:19:51.343170Z",
     "start_time": "2024-04-17T16:19:50.759776Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from src.preprocessing.hatespeech_dataset_querying import prepare_hatespeech_v2_dataset, load_hatespeech_v2_dataset\n",
    "\n",
    "try:\n",
    "    print(run_only_once)\n",
    "except Exception as e:\n",
    "    print(os.getcwd())\n",
    "    os.chdir(\"./../..\")\n",
    "    print(os.getcwd())\n",
    "    run_only_once = \"Dir has already been changed\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zan\\Desktop\\NTNU\\TDT4310_Intelligent_text_analysis_language_understanding\\ITAaLU_project\\notebook\\data_exploration\n",
      "C:\\Users\\Zan\\Desktop\\NTNU\\TDT4310_Intelligent_text_analysis_language_understanding\\ITAaLU_project\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preparing and loading the data",
   "id": "b000fd5a0374044a"
  },
  {
   "cell_type": "code",
   "id": "48ce5f4a-31b2-4eab-a35a-2d2c391e2abf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T16:19:52.836790Z",
     "start_time": "2024-04-17T16:19:52.206005Z"
    }
   },
   "source": [
    "# run if you need to create the preprocessed data file again\n",
    "# prepare_hatespeech_v2_dataset(save=True)\n",
    "df = load_hatespeech_v2_dataset()\n",
    "df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  tweet_id                                               text  \\\n",
       "0      1344794359233998850  You know maybe doing a ‚Äúchallenge‚Äù where I dri...   \n",
       "1      1344794162625916935  RT @thehill: Black transgender woman found dea...   \n",
       "2      1344794094837637121  2021 Goals: Playtest and release Rumrunners. R...   \n",
       "3      1344790842117140483  Guest Co Host: Men Like Us Podcast #StopTheHat...   \n",
       "4      1344788907360190465  üëè Congratulations @AyodejiOsowobi @StandtoEndR...   \n",
       "...                    ...                                                ...   \n",
       "68592  1277310569700196352  Fuck you @Google @GooglePlayDev @Android With ...   \n",
       "68593  1277310293467713536  Being an Arsenal fan is tough. Even people tha...   \n",
       "68594  1277309147697106945  No subs yet? Fuck off man we aren't playing in...   \n",
       "68595  1277309020198633475  Not Manchester United again damn it ü§£ I don't ...   \n",
       "68596  1277308852493524992  FUCKING KNEW IT, OBVIOUSLY HAD TO BE MANCHESTE...   \n",
       "\n",
       "       label topic  \n",
       "0          0   1.0  \n",
       "1          0   1.0  \n",
       "2          0   1.0  \n",
       "3          0   1.0  \n",
       "4          0   1.0  \n",
       "...      ...   ...  \n",
       "68592      1   4.0  \n",
       "68593      1   4.0  \n",
       "68594      1   4.0  \n",
       "68595      2   4.0  \n",
       "68596      0   4.0  \n",
       "\n",
       "[68597 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1344794359233998850</td>\n",
       "      <td>You know maybe doing a ‚Äúchallenge‚Äù where I dri...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1344794162625916935</td>\n",
       "      <td>RT @thehill: Black transgender woman found dea...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1344794094837637121</td>\n",
       "      <td>2021 Goals: Playtest and release Rumrunners. R...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1344790842117140483</td>\n",
       "      <td>Guest Co Host: Men Like Us Podcast #StopTheHat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1344788907360190465</td>\n",
       "      <td>üëè Congratulations @AyodejiOsowobi @StandtoEndR...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68592</th>\n",
       "      <td>1277310569700196352</td>\n",
       "      <td>Fuck you @Google @GooglePlayDev @Android With ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68593</th>\n",
       "      <td>1277310293467713536</td>\n",
       "      <td>Being an Arsenal fan is tough. Even people tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68594</th>\n",
       "      <td>1277309147697106945</td>\n",
       "      <td>No subs yet? Fuck off man we aren't playing in...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68595</th>\n",
       "      <td>1277309020198633475</td>\n",
       "      <td>Not Manchester United again damn it ü§£ I don't ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68596</th>\n",
       "      <td>1277308852493524992</td>\n",
       "      <td>FUCKING KNEW IT, OBVIOUSLY HAD TO BE MANCHESTE...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68597 rows √ó 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T12:10:47.730832Z",
     "start_time": "2024-04-18T12:10:47.718412Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b2fb3c285f9e724d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T16:20:00.077377Z",
     "start_time": "2024-04-17T16:19:54.701255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unhcr/hatespeech-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"unhcr/hatespeech-detection\")"
   ],
   "id": "c54d1ac62cdd3066",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T16:39:52.716401Z",
     "start_time": "2024-04-17T16:20:26.393187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "import tqdm \n",
    "pipe = pipeline(\"text-classification\", model=\"unhcr/hatespeech-detection\", device=\"cuda:0\")  # \n",
    "\n",
    "y_pred = pipe(list(df[\"text\"].values))"
   ],
   "id": "9cfa8ae1ca8de734",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17min 3s\n",
      "Wall time: 19min 26s\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def map_predicted_to_label(y_pred):\n",
    "    y_pred_mapped = []\n",
    "    for json_pair in y_pred:\n",
    "        if json_pair[\"label\"] == \"Normal\":\n",
    "            y_pred_mapped.append(0)\n",
    "        if json_pair[\"label\"] == \"Offensive\":\n",
    "            y_pred_mapped.append(1)\n",
    "        if json_pair[\"label\"] == \"Hate speech\":\n",
    "            y_pred_mapped.append(2)\n",
    "    return y_pred_mapped\n",
    "\n",
    "\n",
    "y_truth = df[\"label\"]\n",
    "mapped_pred = map_predicted_to_label(y_pred)\n",
    "\n",
    "print(classification_report(y_truth[:100], mapped_pred))"
   ],
   "id": "fe09cac753e45dee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c3049475695c4c07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# second model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IMSyPP/hate_speech_en\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"IMSyPP/hate_speech_en\")"
   ],
   "id": "787f5f358488417e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Extracting features",
   "id": "bb025f2c3ed60bc"
  },
  {
   "cell_type": "code",
   "id": "92ac3023",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    porter = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [porter.stem(token) for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"tokenized_text\"] = df[\"text\"].apply(lambda x: preprocess_text(x))\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ba53050",
   "metadata": {},
   "source": [
    "# count occurrences \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['tokenized_text'])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cf4e558",
   "metadata": {},
   "source": [
    "# TODO finish implmentation\n",
    "word_counts = X.sum(axis=0)\n",
    "word_counts_df = pd.DataFrame(word_counts, columns=vectorizer.get_feature_names_out())\n",
    "word_counts_sorted = word_counts_df.transpose().sort_values(by=0, ascending=False)\n",
    "word_counts_sorted"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0d00b1f",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4e95a3a-3dfa-4ed7-b916-5221ed4839c5",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d50bca7e-bb15-4dd2-bdd0-05ceb120f379",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cc9a909-2758-4bb7-b081-cbce7b2d38a5",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3392a99-e174-4f67-a6b5-9e1e2f60b81e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "362f8d26-a511-44d1-a684-6bcf4e84f9c6",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
