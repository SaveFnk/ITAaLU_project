{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## preprocessing\n",
    "- Tokenization\n",
    "- Stop Word Removal\n",
    "- Lemmatization\n",
    "- Stemming"
   ],
   "id": "7d59c8f900477131"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:12:18.995484Z",
     "start_time": "2024-04-20T18:12:18.992835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ],
   "id": "c474d2341aa4da4e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/saveriofnk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/saveriofnk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/saveriofnk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:12:19.009157Z",
     "start_time": "2024-04-20T18:12:19.005550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Define your preprocessing function\n",
    "def preprocess_text(text_series):\n",
    "    # Tokenization\n",
    "    tokens = text_series.apply(word_tokenize)\n",
    "\n",
    "    # Lowercase and strip\n",
    "    tokens = tokens.apply(lambda x: [word.lower().strip() for word in x])\n",
    "\n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = tokens.apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = tokens.apply(lambda x: [lemmatizer.lemmatize(word, pos=\"v\") for word in x])\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = tokens.apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "    # Join tokens into a single string\n",
    "    tokens = tokens.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    return tokens"
   ],
   "id": "1118f0cc1fa69d21",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the Dataset",
   "id": "ebe317e25e172498"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:12:19.019096Z",
     "start_time": "2024-04-20T18:12:19.017275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.preprocessing.hatespeech_dataset_querying import prepare_hatespeech_v2_dataset, load_hatespeech_v2_dataset, split_hatespeech_v2_dataset\n"
   ],
   "id": "ecbc7baffeaf6129",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:12:19.673102Z",
     "start_time": "2024-04-20T18:12:19.019848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#RUN if you don't have the test and train dataset files\n",
    "#create test and train dataset\n",
    "df_train, df_test = split_hatespeech_v2_dataset(\"../data/hatespeech_v2/prepared_hatespeech_v2.csv\")"
   ],
   "id": "aa7707c30baeb7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train data to: ../data/hatespeech_v2/train_hatespeech_v2.csv\n",
      "Saved test data to: ../data/hatespeech_v2/test_hatespeech_v2.csv\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:12:19.868953Z",
     "start_time": "2024-04-20T18:12:19.674107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the test and train dataset\n",
    "train_df = pd.read_csv('../data/hatespeech_v2/train_hatespeech_v2.csv', sep=',')\n",
    "test_df = pd.read_csv('../data/hatespeech_v2/test_hatespeech_v2.csv', sep=',')\n"
   ],
   "id": "cb26a519933221ba",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Extraction + TD-IDF + Naive Bayes",
   "id": "fd9a265d7c839c46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:12:19.871949Z",
     "start_time": "2024-04-20T18:12:19.870057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.exceptions import UndefinedMetricWarning"
   ],
   "id": "1923d4d64026a838",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:12:37.312842Z",
     "start_time": "2024-04-20T18:12:19.872342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the training data into X_train and y_train\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['label']\n",
    "\n",
    "# Split the test data into X_test and y_test\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Suppress UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', FunctionTransformer(preprocess_text, validate=False)),  # Apply preprocessing function\n",
    "    ('vectorizer', TfidfVectorizer()),  # Vectorize/extract features using TF-IDF\n",
    "    ('classifier', MultinomialNB())  # Train a Naive Bayes classifier\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "nb_y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "nb_accuracy = accuracy_score(y_test, nb_y_pred)\n",
    "nb_report = classification_report(y_test, nb_y_pred, digits=4)\n",
    "\n",
    "print(f\"Accuracy: {nb_accuracy * 100:.2f}%\")  # Improve formatting to two decimal places\n",
    "print(\"Classification report:\\n\", nb_report)  # Remove unnecessary f-string"
   ],
   "id": "d55f91fb50b4260",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.50%\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8039    0.9994    0.8911     10873\n",
      "         1.0     0.8762    0.0702    0.1301      2520\n",
      "         2.0     0.0000    0.0000    0.0000       327\n",
      "\n",
      "    accuracy                         0.8050     13720\n",
      "   macro avg     0.5600    0.3566    0.3404     13720\n",
      "weighted avg     0.7980    0.8050    0.7301     13720\n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:12:37.330276Z",
     "start_time": "2024-04-20T18:12:37.313747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "filename = '../data/model/nb_model_TFIDF_01.sav'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))"
   ],
   "id": "ecd92e61302efc85",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Extraction + TD-IDF + SVM",
   "id": "4f82c084a1ff4fef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:12:37.332928Z",
     "start_time": "2024-04-20T18:12:37.331033Z"
    }
   },
   "cell_type": "code",
   "source": "from sklearn.svm import SVC",
   "id": "28586040da02be92",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:23:19.686579Z",
     "start_time": "2024-04-20T18:12:37.333435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', FunctionTransformer(preprocess_text, validate=False)),  # Apply preprocessing function\n",
    "    ('vectorizer', TfidfVectorizer()),  # Vectorize/extract features using TF-IDF\n",
    "    ('classifier', SVC())  # Train an SVM classifier\n",
    "])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "svm_y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
    "svm_report = classification_report(y_test, svm_y_pred, digits=4)\n",
    "\n",
    "print(f\"Accuracy: {svm_accuracy * 100:.2f}%\")  # Improve formatting to two decimal places\n",
    "print(\"Classification report:\\n\", svm_report)  # Remove unnecessary f-string\n"
   ],
   "id": "7e42e1bda264c20c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.97%\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9392    0.9725    0.9555     10873\n",
      "         1.0     0.8314    0.8040    0.8174      2520\n",
      "         2.0     0.7500    0.0550    0.1026       327\n",
      "\n",
      "    accuracy                         0.9197     13720\n",
      "   macro avg     0.8402    0.6105    0.6252     13720\n",
      "weighted avg     0.9148    0.9197    0.9098     13720\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:23:19.689979Z",
     "start_time": "2024-04-20T18:23:19.687951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #save the model in a file pickle\n",
    "# import pickle\n",
    "# filename = '../data/model/svm_model_TFIDF_01.sav'\n",
    "# pickle.dump(pipeline, open(filename, 'wb'))\n",
    "# \n",
    "# #save the model only not the entire pipeline in a file pickle\n",
    "# #filename = 'svm_model.sav'\n",
    "# #pickle.dump(pipeline.named_steps['classifier'], open(filename, 'wb'))"
   ],
   "id": "32491d345b0071ec",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Extraction + TD-IDF + Random Forest",
   "id": "6a0ce4c5511cee7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:23:19.694693Z",
     "start_time": "2024-04-20T18:23:19.693443Z"
    }
   },
   "cell_type": "code",
   "source": "from sklearn.ensemble import RandomForestClassifier",
   "id": "114ccdf56bd40173",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:24:54.971493Z",
     "start_time": "2024-04-20T18:23:19.695063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', FunctionTransformer(preprocess_text, validate=False)),  # Apply preprocessing function\n",
    "    ('vectorizer', TfidfVectorizer()),  # Vectorize/extract features using TF-IDF\n",
    "    ('classifier', RandomForestClassifier())  # Train a Random Forest classifier\n",
    "])\n",
    "\n",
    "# Suppress UndefinedMetricWarning for Random Forest\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "rf_y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "rf_report = classification_report(y_test, rf_y_pred, digits=4)\n",
    "\n",
    "print(f\"Accuracy: {rf_accuracy * 100:.2f}%\")  # Improve formatting to two decimal places\n",
    "print(\"Classification report:\\n\", rf_report)  # Remove unnecessary f-string\n"
   ],
   "id": "22f691b3752f9545",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.60%\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9172    0.9791    0.9472     10873\n",
      "         1.0     0.8441    0.7067    0.7693      2520\n",
      "         2.0     1.0000    0.0092    0.0182       327\n",
      "\n",
      "    accuracy                         0.9060     13720\n",
      "   macro avg     0.9204    0.5650    0.5782     13720\n",
      "weighted avg     0.9057    0.9060    0.8924     13720\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:24:54.973489Z",
     "start_time": "2024-04-20T18:24:54.972064Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f93d1a9fa4bcdcbf",
   "outputs": [],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
