{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## preprocessing\n",
    "- Tokenization\n",
    "- Stop Word Removal\n",
    "- Lemmatization\n",
    "- Stemming"
   ],
   "id": "7d59c8f900477131"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:20:18.988199Z",
     "start_time": "2024-04-17T12:20:18.571980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ],
   "id": "c474d2341aa4da4e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/saveriofnk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/saveriofnk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/saveriofnk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:20:19.439113Z",
     "start_time": "2024-04-17T12:20:19.436127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Define your preprocessing function\n",
    "def preprocess_text(text_series):\n",
    "    # Tokenization\n",
    "    tokens = text_series.apply(word_tokenize)\n",
    "\n",
    "    # Lowercase and strip\n",
    "    tokens = tokens.apply(lambda x: [word.lower().strip() for word in x])\n",
    "\n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = tokens.apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = tokens.apply(lambda x: [lemmatizer.lemmatize(word, pos=\"v\") for word in x])\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = tokens.apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "    # Join tokens into a single string\n",
    "    tokens = tokens.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    return tokens"
   ],
   "id": "1118f0cc1fa69d21",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the Dataset",
   "id": "ebe317e25e172498"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:20:21.183734Z",
     "start_time": "2024-04-17T12:20:21.058371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.preprocessing.hatespeech_dataset_querying import prepare_hatespeech_v2_dataset, load_hatespeech_v2_dataset, split_hatespeech_v2_dataset\n"
   ],
   "id": "ecbc7baffeaf6129",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:20:22.501738Z",
     "start_time": "2024-04-17T12:20:21.839323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#RUN if you don't have the test and train dataset files\n",
    "#create test and train dataset\n",
    "df_train, df_test = split_hatespeech_v2_dataset(\"../data/hatespeech_v2/prepared_hatespeech_v2.csv\")"
   ],
   "id": "aa7707c30baeb7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train data to: ../data/hatespeech_v2/train_hatespeech_v2.csv\n",
      "Saved test data to: ../data/hatespeech_v2/test_hatespeech_v2.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:23:27.650585Z",
     "start_time": "2024-04-17T12:23:27.434572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the test and train dataset\n",
    "train_df = pd.read_csv('../data/hatespeech_v2/train_hatespeech_v2.csv', sep=',')\n",
    "test_df = pd.read_csv('../data/hatespeech_v2/test_hatespeech_v2.csv', sep=',')\n"
   ],
   "id": "cb26a519933221ba",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Extraction + TD-IDF + Naive Bayes",
   "id": "fd9a265d7c839c46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:23:31.720161Z",
     "start_time": "2024-04-17T12:23:31.717435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.exceptions import UndefinedMetricWarning"
   ],
   "id": "1923d4d64026a838",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:24:51.543862Z",
     "start_time": "2024-04-17T12:24:33.364006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the training data into X_train and y_train\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['label']\n",
    "\n",
    "# Split the test data into X_test and y_test\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Suppress UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', FunctionTransformer(preprocess_text, validate=False)),  # Apply preprocessing function\n",
    "    ('vectorizer', TfidfVectorizer()),  # Vectorize/extract features using TF-IDF\n",
    "    ('classifier', MultinomialNB())  # Train a Naive Bayes classifier\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "nb_y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "nb_accuracy = accuracy_score(y_test, nb_y_pred)\n",
    "nb_report = classification_report(y_test, nb_y_pred)\n",
    "\n",
    "print(f\"Accuracy: {nb_accuracy * 100:.2f}%\")  # Improve formatting to two decimal places\n",
    "print(\"Classification report:\\n\", nb_report)  # Remove unnecessary f-string"
   ],
   "id": "d55f91fb50b4260",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.50%\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      1.00      0.89     10873\n",
      "         1.0       0.88      0.07      0.13      2520\n",
      "         2.0       0.00      0.00      0.00       327\n",
      "\n",
      "    accuracy                           0.80     13720\n",
      "   macro avg       0.56      0.36      0.34     13720\n",
      "weighted avg       0.80      0.80      0.73     13720\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "filename = '../data/model/nb_model_TFIDF_01.sav'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))"
   ],
   "id": "ecd92e61302efc85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Extraction + TD-IDF + SVM",
   "id": "4f82c084a1ff4fef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from sklearn.svm import SVC",
   "id": "28586040da02be92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', FunctionTransformer(preprocess_text, validate=False)),  # Apply preprocessing function\n",
    "    ('vectorizer', TfidfVectorizer()),  # Vectorize/extract features using TF-IDF\n",
    "    ('classifier', SVC())  # Train an SVM classifier\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "svm_y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
    "svm_report = classification_report(y_test, svm_y_pred)\n",
    "\n",
    "print(f\"Accuracy: {svm_accuracy * 100:.2f}%\")  # Improve formatting to two decimal places\n",
    "print(\"Classification report:\\n\", svm_report)  # Remove unnecessary f-string\n"
   ],
   "id": "7e42e1bda264c20c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#save the model in a file pickle\n",
    "import pickle\n",
    "filename = '../data/model/svm_model_TFIDF_01.sav'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))\n",
    "\n",
    "#save the model only not the entire pipeline in a file pickle\n",
    "#filename = 'svm_model.sav'\n",
    "#pickle.dump(pipeline.named_steps['classifier'], open(filename, 'wb'))"
   ],
   "id": "32491d345b0071ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#set(svm_y_pred)\n",
    "#predict my sentence\n",
    "# Preprocess the single sentence\n",
    "# Convert the sentence into a pandas Series with a single element\n",
    "single_series = pd.Series(\"fuck Italian\")\n",
    "\n",
    "preprocessed_sentence = preprocess_text(single_series)\n",
    "\n",
    "# Vectorize the preprocessed sentence\n",
    "vectorized_sentence = pipeline.named_steps['vectorizer'].transform([preprocessed_sentence])\n",
    "\n",
    "# Predict the label\n",
    "predicted_label = pipeline.predict(vectorized_sentence)\n",
    "\n",
    "print(predicted_label)"
   ],
   "id": "9a91d787afd80ee8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Extraction + TD-IDF + Random Forest",
   "id": "6a0ce4c5511cee7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from sklearn.ensemble import RandomForestClassifier",
   "id": "114ccdf56bd40173",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', FunctionTransformer(preprocess_text, validate=False)),  # Apply preprocessing function\n",
    "    ('vectorizer', TfidfVectorizer()),  # Vectorize/extract features using TF-IDF\n",
    "    ('classifier', RandomForestClassifier())  # Train a Random Forest classifier\n",
    "])\n",
    "\n",
    "# Suppress UndefinedMetricWarning for Random Forest\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "rf_y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "rf_report = classification_report(y_test, rf_y_pred)\n",
    "\n",
    "print(f\"Accuracy: {rf_accuracy * 100:.2f}%\")  # Improve formatting to two decimal places\n",
    "print(\"Classification report:\\n\", rf_report)  # Remove unnecessary f-string\n"
   ],
   "id": "22f691b3752f9545",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparison with Dataset Owner model",
   "id": "ec370793806ff80d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:38:14.460456Z",
     "start_time": "2024-04-17T12:38:14.024579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# #from transformers import pipeline\n",
    "# \n",
    "# #pipe = pipeline(\"text-classification\", model=\"ctoraman/hate-speech-berturk\")\n",
    "# \n",
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ctoraman/hate-speech-berturk\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"ctoraman/hate-speech-berturk\")"
   ],
   "id": "c5025ac2449706cd",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T13:27:39.317985Z",
     "start_time": "2024-04-17T13:27:39.315179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #use the model on test dataset and get the accuracy\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from transformers import TextClassificationPipeline\n",
    "# \n",
    "# # Create a TextClassificationPipeline\n",
    "# pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "# \n",
    "# # Predict\n",
    "# predictions = pipe(X_test.tolist())\n",
    "# \n",
    "# # Extract the predicted labels\n",
    "# predictions "
   ],
   "id": "de0966f7f441e015",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f93d1a9fa4bcdcbf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
